{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge datasets==1.4.1 transformers==4.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781f126",
   "metadata": {},
   "source": [
    "## preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d05ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Using custom data configuration default-515c8d33f7b5171f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/ec2-user/.cache/huggingface/datasets/csv/default-515c8d33f7b5171f/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2134a2bf7a0e4eb485f99d53726e9e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/csv/default-515c8d33f7b5171f/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "## data process \n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "df_article_summary_full = pd.read_parquet('../../../data-v1/article_summary.parquet', engine='pyarrow')\n",
    "df_article_summary_full[['original_text','summary']].to_csv('./total.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset('csv', data_files='./total.csv',split='train')\n",
    "dataset =  dataset.rename_column(\"original_text\", \"article\")\n",
    "dataset =  dataset.rename_column(\"summary\", \"summarization\")\n",
    "#train test split\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6f711f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'summarization'],\n",
       "        num_rows: 13500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'summarization'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abf7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4327736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = './demo_data/SUMMARY.hk01'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "print(\"The new directory is created!\")\n",
    "\n",
    "#dataset['train'].to_json(os.path.join(path,'train.json'),orient = 'records')\n",
    "#dataset['test'].to_json(os.path.join(path,'test.json'), orient = 'records')\n",
    "\n",
    "# use csv file to test \n",
    "dataset['train'].to_csv(os.path.join(path,'train.csv'))\n",
    "dataset['test'].to_csv(os.path.join(path,'test.csv'))\n",
    "dataset['test'].to_csv(os.path.join(path,'dev.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8c3d0",
   "metadata": {},
   "source": [
    "## run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8accaa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"run_gen_v2.py\", line 11, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n"
     ]
    }
   ],
   "source": [
    "#!python run_gen.py --model_path 'fnlp/cpt-base' --dataset adgen --data_dir demo_data\n",
    "!python run_gen_v2.py --model_path 'fnlp/cpt-base' --dataset hk01 --data_dir demo_data --epoch '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6597de65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ [ 类 型, 上 衣 ], [ 版 型, 宽 松 ], [ 风 格, 复 古 ], [ 图 案, 复 古 ], [ 衣 样 式, 针 织 衫 ] ]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finetune/modeling_cpt.py\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer=BertTokenizer.from_pretrained('output/adgen/2')\n",
    "ids=tokenizer.encode(\"[[类型, 上衣], [版型, 宽松], [风格, 复古], [图案, 复古], [衣样式, 针织衫]]\")\n",
    "tokenizer.decode(ids,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa95233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu_metric.py\toutput\t     README.md\t  run_gen.py  Untitled.ipynb\n",
      "demo_data\t__pycache__  run_bleu.py  runs\t      utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0bd3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results.json   special_tokens_map.json  trainer_state.json\tvocab.txt\n",
      "config.json\t   test_generations.txt     training_args.bin\n",
      "pytorch_model.bin  tokenizer_config.json    train_results.json\n"
     ]
    }
   ],
   "source": [
    "!ls output/adgen/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e84ff380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03574327267496428\n"
     ]
    }
   ],
   "source": [
    "!python run_bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d7d5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import load_json\n",
    "from transformers import BertTokenizer\n",
    "from bleu_metric import Metric\n",
    "\n",
    "#需要计算bleu的文本文件所在的目录\n",
    "arch='output'\n",
    "tokenizer=BertTokenizer.from_pretrained('output/adgen/2')\n",
    "dataset='adgen'\n",
    "test_set=load_json('demo_data/SUMMARY.adgen/test.json')\n",
    "\n",
    "labels=[]\n",
    "for data in test_set:\n",
    "    ids=tokenizer.encode(data['summarization'])\n",
    "    labels.append(tokenizer.decode(ids,skip_special_tokens=True))\n",
    "    \n",
    "labels=[[label.strip().split(' ')] for label in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed76c7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summarization': '这款阔腿裤，整体设计简约利落，时尚的阔腿款式带来鲜明的几何设计美感，褪去传统装束的厚重与臃肿，更具轻盈美感。 搭配七分裤长修饰出挺拔的腿部线条，气质的格纹图案不显单调，尽显女性优雅气质。 斜门襟设计潮流出众，让你时刻保持动人的女性风采。',\n",
       " 'article': '[[类型, 裤], [风格, 简约], [风格, 潮], [图案, 格子], [图案, 几何], [图案, 线条], [裤长, 七分裤], [裤型, 阔腿裤]]'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e563078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(test_set[0]['article'], max_length=128, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9cbb89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 138, 138, 5102, 1798, 117, 6175, 140, 117, 138, 7599, 3419, 117, 5042, 5276, 140, 117, 138, 7599, 3419, 117, 4060, 140, 117, 138, 1745, 3428, 117, 3419, 2094, 140, 117, 138, 1745, 3428, 117, 1126, 862, 140, 117, 138, 1745, 3428, 117, 5296, 3340, 140, 117, 138, 6175, 7270, 117, 673, 1146, 6175, 140, 117, 138, 6175, 1798, 117, 7333, 5597, 6175, 140, 140, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7190d",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e56d2259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[类型, 裤], [风格, 简约], [风格, 潮], [图案, 格子], [图案, 几何], [图案, 线条], [裤长, 七分裤], [裤型, 阔腿裤]]\n",
      "约 ， 简 约 自 然 简 约 的 设 计 感 强 烈 的 感 觉, 简 单 的 复 古 典 主 义 感 觉 到 极 致 感 觉 的 美 感,, 图 案, 线 条 。 。 。 图 案 的 几 何 图 案 ， 格 子,, [ 风 格, 格 子 线 条, 几 何 线 条 ， 图 案 、 线 条 图 案 。 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from modeling_cpt import CPTModel, CPTForConditionalGeneration\n",
    "\n",
    "model = CPTForConditionalGeneration.from_pretrained(\"output/adgen/2\")\n",
    "tokenizer = BertTokenizer.from_pretrained('output/adgen/2')\n",
    "\n",
    "#model_inputs = tokenizer(test_set[0]['article'], max_length=128, padding=\"max_length\", truncation=True,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c3565ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  自有紀錄以來，巴約掛毯已在法國「生活」了950年，從未離開過「家門」。有報道指，法國總統馬克龍答應首次將巴約掛氈借出海外，而獲得這份「借來的厚禮」，就是正忙於脫歐的英國。按官方說法，這次「掛毯外交」旨在展示英法友好關係。但在友誼背後，卻揭示馬克龍「一啖砂糖一啖屎」、軟硬兼施的外交哲學SPLIT中國有一幅傳世名畫《清明上河圖》，法國也有，而它叫作「巴約掛氈」（Bayeux Tapestry）。 這幅近千年歷史的刺鏽畫上11世紀諾曼征服英格蘭的黑斯廷斯戰役（Battle of Hastings），極具歷史價值。 \n",
      "result:   ， 法 国 国 王 的 马 克 龍 是 英 国 王 朝 的 王 朝 军 队 的 一 次 巴 约 [SEP]\n",
      "CPU times: user 18.4 s, sys: 0 ns, total: 18.4 s\n",
      "Wall time: 5.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#input_text = \"[[类型, 裤], [风格, 简约], [风格, 潮], [图案, 格子], [图案, 几何], [图案, 线条], [裤长, 七分裤], [裤型, 阔腿裤]]\"\n",
    "input_text = \"自有紀錄以來，巴約掛毯已在法國「生活」了950年，從未離開過「家門」。有報道指，法國總統馬克龍答應首次將巴約掛氈借出海外，而獲得這份「借來的厚禮」，就是正忙於脫歐的英國。按官方說法，這次「掛毯外交」旨在展示英法友好關係。但在友誼背後，卻揭示馬克龍「一啖砂糖一啖屎」、軟硬兼施的外交哲學SPLIT中國有一幅傳世名畫《清明上河圖》，法國也有，而它叫作「巴約掛氈」（Bayeux Tapestry）。 這幅近千年歷史的刺鏽畫上11世紀諾曼征服英格蘭的黑斯廷斯戰役（Battle of Hastings），極具歷史價值。 \"\n",
    "print(\"input: \",input_text)\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs['input_ids'][0]))\n",
    "outputs = model.generate(inputs['input_ids'], max_length=250, do_sample=True, top_p=0.95, top_k=60)\n",
    "generated = tokenizer.decode(outputs[0])[prompt_length+1:]\n",
    "\n",
    "print(\"result: \",generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a6226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
