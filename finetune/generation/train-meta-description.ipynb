{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81232c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: datasets==1.4.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (1.4.1)\n",
      "Requirement already satisfied: transformers==4.4.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (4.4.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (0.8)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (2.0.2)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (2.26.0)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (4.49.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (2021.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (4.8.1)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (5.0.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from datasets==1.4.1) (0.0.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.4.2) (21.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.4.2) (0.0.46)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.4.2) (3.3.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.4.2) (2020.11.13)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers==4.4.2) (0.10.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.4.1) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.4.1) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.4.1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.4.1) (2021.10.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->datasets==1.4.1) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->datasets==1.4.1) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers==4.4.2) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets==1.4.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas->datasets==1.4.1) (2021.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge datasets==1.4.1 transformers==4.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80bbba2",
   "metadata": {},
   "source": [
    "## preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de35384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = './demo_data/SUMMARY.hk01meta'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "print(\"The new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea85263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data process \n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "df_article_summary_full = pd.read_parquet('/home/ec2-user/SageMaker/hk01/data/meta_description.parquet', engine='pyarrow')\n",
    "\n",
    "df_article_summary_full[['original_text','meta_descrption']].to_csv('./demo_data/SUMMARY.hk01meta/total.csv',index=False)\n",
    "\n",
    "total_data = pd.read_csv('./demo_data/SUMMARY.hk01meta/total.csv')\n",
    "x = total_data[-total_data['meta_descrption'].isnull()]\n",
    "x.columns = ['article','summarization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e81b38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use csv file to test \n",
    "x[:1000].to_csv(os.path.join(path,'train.csv'),index=False,encoding='utf-8')\n",
    "x[1000:1200].to_csv(os.path.join(path,'test.csv'),index=False,encoding='utf-8')\n",
    "x[1200:1400].to_csv(os.path.join(path,'dev.csv'),index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47cc7f1",
   "metadata": {},
   "source": [
    "## run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d8510fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4fa5bad4c3e38839\n",
      "Reusing dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-4fa5bad4c3e38839/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c)\n",
      "dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'summarization'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'summarization'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'summarization'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "11/02/2021 07:39:14 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/02/2021 07:39:14 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='output/hk01meta/4', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Nov02_07-39-14_ip-172-16-2-197', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.NO: 'no'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=4000, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='output/hk01meta/4', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
      "Didn't find file /home/ec2-user/SageMaker/hk01/CPT/endpoint/6/added_tokens.json. We won't load it.\n",
      "Didn't find file /home/ec2-user/SageMaker/hk01/CPT/endpoint/6/tokenizer.json. We won't load it.\n",
      "loading file /home/ec2-user/SageMaker/hk01/CPT/endpoint/6/vocab.txt\n",
      "loading file None\n",
      "loading file /home/ec2-user/SageMaker/hk01/CPT/endpoint/6/special_tokens_map.json\n",
      "loading file /home/ec2-user/SageMaker/hk01/CPT/endpoint/6/tokenizer_config.json\n",
      "loading file None\n",
      "loading configuration file /home/ec2-user/SageMaker/hk01/CPT/endpoint/6/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"fnlp/cpt-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"CPTForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 24,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file /home/ec2-user/SageMaker/hk01/CPT/endpoint/6/pytorch_model.bin\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"run_gen_v2.py\", line 121, in <module>\n",
      "    model = CPTForConditionalGeneration.from_pretrained(model_args.model_name_or_path)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py\", line 1058, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/home/ec2-user/SageMaker/CPT/finetune/generation/../modeling_cpt.py\", line 973, in __init__\n",
      "    self.model = CPTModel(config)\n",
      "  File \"/home/ec2-user/SageMaker/CPT/finetune/generation/../modeling_cpt.py\", line 833, in __init__\n",
      "    self.encoder = BertModel(encoder_config, add_pooling_layer=False)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 847, in __init__\n",
      "    self.encoder = BertEncoder(config)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 517, in __init__\n",
      "    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 517, in <listcomp>\n",
      "    self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 435, in __init__\n",
      "    self.attention = BertAttention(config)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 356, in __init__\n",
      "    self.output = BertSelfOutput(config)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 341, in __init__\n",
      "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 83, in __init__\n",
      "    self.reset_parameters()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/linear.py\", line 86, in reset_parameters\n",
      "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/init.py\", line 384, in kaiming_uniform_\n",
      "    return tensor.uniform_(-bound, bound)\n",
      "KeyboardInterrupt\n",
      "CPU times: user 65.1 ms, sys: 82.9 ms, total: 148 ms\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_gen_v2.py --model_path 'fnlp/cpt-large' --dataset hk01meta --data_dir demo_data --epoch '1' --batch_size '4' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb2b45",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8a8b4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-26371a9e5916b36a\n",
      "Reusing dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-26371a9e5916b36a/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from modeling_cpt import CPTModel, CPTForConditionalGeneration\n",
    "\n",
    "model = CPTForConditionalGeneration.from_pretrained(\"output/hk01meta/2\")\n",
    "tokenizer = BertTokenizer.from_pretrained('output/hk01meta/2')\n",
    "dataset = load_dataset('csv', data_files='./demo_data/SUMMARY.hk01meta/test.csv',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d665758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def print_result(idx):\n",
    "    input_text = dataset[idx]['article']\n",
    "    print(\"input: \",input_text)\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\",max_length=512)\n",
    "\n",
    "    #prompt_length = len(tokenizer.decode(inputs['input_ids'][0]))\n",
    "    #outputs = model.generate(inputs['input_ids'], max_length=64, do_sample=True, top_p=0.95, top_k=60)\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=64, top_p=0.95)\n",
    "    generated = tokenizer.decode(outputs[0])\n",
    "\n",
    "    print(\"prediction result: \",generated)\n",
    "    \n",
    "    print ('label: ',dataset[idx]['summarization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b911ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  【Pokemon 寶可夢 劍盾 攻略】除了主線、儲圖鑑、育成和對戰之外，原來劍盾中還有不少NPC支線事件 / 任務，完成後可拿到一些不錯的道具。 草路鎮尋寶事件 草路鎮內有一個黑肉小蘿莉跟你說有個尋寶迷題，解開後可獲得寶物，當然要把它解開了。謎面是「最初為草，由強至弱，追尋兩次」，簡單來說就是先找到草的石碑，然後再找它剋制的屬性之石碑兩次（即是水，然後是火），將三個石碑順序調查一次後就可獲得寶物（達人帶）了。 火的石碑在城鎮最右邊 拿到寶物後小蘿莉會說你太厲害了（意味深） 水舟鎮送外賣任務 （如果無法觸發可試試完成主線後再來）和水舟鎮的海鮮餐廳廚師對話，他們請你幫忙送外賣，總共要送3次，而每一次廚師都會說忘了問地址要玩家自己找目的地lol不過當然會有提示，而且地點 全都在水舟鎮 內。每次送完都會獲得道具，送完三次後可獲得「幸運蛋」，效果是持有的寶可夢獲得之經驗值提升。 第一次送外賣的提示是火車聲 十分簡單就是火車站旁的屋子 你唔係外賣仔！！ 第二次的提示是綿綿芙的叫聲 第二次的提示是綿綿芙的叫聲 答案是靠近樓梯的屋子，這家人養了一隻綿綿芙 答案是靠近樓梯的屋子，這家人養了一隻綿綿芙 答案是靠近樓梯的屋子，這家人養了一隻綿綿芙 答案是靠近樓梯的屋子，這家人養了一隻綿綿芙 獲得兩個巨大金珠作報酬 第三次的提示是綠色的屋頂 第三次的提示是綠色的屋頂 綠色屋頂是沒有但有帳篷頂，在市集那邊 綠色屋頂是沒有但有帳篷頂，在市集那邊 最後再回去和廚師對話可得幸運蛋 最後再回去和廚師對話可得幸運蛋 效果是攜帶的寶可夢獲得經驗值增加，但是對經驗糖果無效，用途有限。 機擎市尋找泡沫栗鼠 在機擎市上層酒店再往左少許有一對站在貨櫃前的雙胞胎，和左邊那位對話他會說他的泡沫栗鼠走失了，見義勇為的主角當然要幫他找回來啦！他說他的泡沫栗鼠喜歡水窪和口哨，原來是走了去道館左手邊的噴水池，在那裡吹一下口哨（按左Analog）便可以找回泡沫栗鼠。之後回去和小男孩對話可獲得「爽喉噴霧」作為謝禮。 拳關市送信任務 第一次來到拳關市時，可以在火車站旁邊的樓梯上去，能在一個小女孩NPC手上接過一封信，請你交給舞姿鎮的法蘭克。在舞姿鎮找到法蘭克時卻發現他是個大叔，收到信時說很懷念，原來信是他多年前還在孤兒院時的玩伴「寶拉」寫的；但後來法蘭克離開了孤兒院（現在連孫兒都有了，生活應該很好），之後就沒再和寶拉聯絡了，而且離開前還聽說寶拉生病了，問主角寶拉現在可好...送完信後回去拿到信的位置，小女孩NPC已經不見了，但調查她原本所站的位置時，會獲得一塊 「靈界之布」 ，還會聽到一句對白「謝謝你把信送過去」，說到這裡大家都知道這怎樣一個故事了吧... 法蘭克就住在舞姿鎮最左手面的屋子 真是個悲傷的故事 戰競鎮偵探事件 在戰競鎮中間廣場右側的愛奧尼亞酒店二樓客房可以觸發劇情，會上演一段以攪笑為主的偵探劇場，玩家要幫忙找出吃掉酒店樹果的犯人。和三個「疑犯」對話後真犯人貪心栗鼠就會現身，指証牠是犯人後獲得道具「廣角鏡」。 01宅民黨\n",
      "prediction result:  [SEP] [CLS] 【 pokemon 寶 可 夢 劍 盾 攻 略 】 除 了 主 線 、 儲 圖 鑑 、 育 成 和 對 戰 之 外 ， 原 來 劍 盾 中 還 有 不 少 npc 支 線 事 件 / 任 務 ， 完 成 後 可 拿 到 一 些 不 錯 的 道 具 。 [SEP]\n",
      "label:  【Pokemon 寶可夢 劍盾 攻略】除了主線、儲圖鑑、育成和對戰之外，原來劍盾中還有不少NPC支線事件 / 任務，完成後可拿到一些不錯的道具。\n"
     ]
    }
   ],
   "source": [
    "print_result(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e2b45",
   "metadata": {},
   "source": [
    "## 增强训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b0b0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4fa5bad4c3e38839\n",
      "Reusing dataset csv (/home/ec2-user/.cache/huggingface/datasets/csv/default-4fa5bad4c3e38839/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c)\n",
      "dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'summarization'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'summarization'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'summarization'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "11/02/2021 07:46:26 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/02/2021 07:46:26 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='output/hk01meta/4', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.EPOCH: 'epoch'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Nov02_07-46-26_ip-172-16-2-197', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_strategy=<IntervalStrategy.NO: 'no'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=4000, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='output/hk01meta/4', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=False, predict_with_generate=True)\n",
      "Didn't find file output/hk01meta/2/added_tokens.json. We won't load it.\n",
      "Didn't find file output/hk01meta/2/tokenizer.json. We won't load it.\n",
      "loading file output/hk01meta/2/vocab.txt\n",
      "loading file None\n",
      "loading file output/hk01meta/2/special_tokens_map.json\n",
      "loading file output/hk01meta/2/tokenizer_config.json\n",
      "loading file None\n",
      "loading configuration file output/hk01meta/2/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"fnlp/cpt-large\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"CPTForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"decoder_start_token_id\": 102,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 24,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"forced_eos_token_id\": 102,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file output/hk01meta/2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing CPTForConditionalGeneration.\n",
      "\n",
      "All the weights of CPTForConditionalGeneration were initialized from the model checkpoint at output/hk01meta/2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CPTForConditionalGeneration for predictions without further training.\n",
      "input shape: ---- 512\n",
      "  0%|                                                     | 0/1 [00:00<?, ?ba/s]input shape: ---- 512\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:11<00:00, 11.53s/ba]\n",
      "input shape: ---- 512\n",
      "  0%|                                                     | 0/1 [00:00<?, ?ba/s]input shape: ---- 512\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.55s/ba]\n",
      "input shape: ---- 482\n",
      "  0%|                                                     | 0/1 [00:00<?, ?ba/s]input shape: ---- 482\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.16s/ba]\n",
      "200\n",
      "The following columns in the training set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 250\n",
      "100%|█████████████████████████████████████████| 250/250 [02:16<00:00,  1.80it/s]***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:01<00:36,  1.32it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:03<00:48,  1.02s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:04<00:52,  1.14s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:07<01:08,  1.53s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:08<01:06,  1.51s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:10<01:12,  1.69s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:12<01:13,  1.76s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:14<01:11,  1.75s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:15<01:07,  1.69s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:17<01:02,  1.61s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:19<01:06,  1.75s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:21<01:08,  1.86s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:23<01:11,  1.98s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:25<01:08,  1.94s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:27<01:02,  1.85s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:28<01:00,  1.84s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:30<00:55,  1.74s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:31<00:51,  1.65s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:33<00:50,  1.68s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:35<00:47,  1.64s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:37<00:49,  1.78s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:39<00:47,  1.77s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:41<00:47,  1.83s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:42<00:43,  1.76s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:44<00:41,  1.73s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:46<00:46,  2.00s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:48<00:43,  1.96s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:50<00:38,  1.81s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:52<00:36,  1.83s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:53<00:32,  1.72s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:55<00:31,  1.74s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:57<00:30,  1.80s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:59<00:28,  1.78s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [01:00<00:26,  1.74s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [01:02<00:23,  1.65s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [01:03<00:22,  1.69s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [01:05<00:20,  1.68s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [01:07<00:19,  1.79s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [01:09<00:18,  1.88s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [01:11<00:16,  1.88s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [01:13<00:15,  1.88s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [01:15<00:13,  1.91s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [01:16<00:10,  1.71s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [01:17<00:07,  1.55s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [01:19<00:05,  1.49s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [01:20<00:04,  1.53s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [01:23<00:03,  1.81s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [01:25<00:01,  1.83s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.9852505326271057, 'eval_rouge-1': 66.4208, 'eval_rouge-2': 55.3232, 'eval_rouge-l': 62.4642, 'eval_gen_len': 66.975, 'eval_runtime': 94.2494, 'eval_samples_per_second': 2.122, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 250/250 [03:50<00:00,  1.80it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [01:32<00:00,  1.76s/it]\u001b[AThe following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 4\n",
      "\n",
      "51it [01:34,  3.64s/it]                                                         \u001b[A\n",
      "52it [01:36,  3.01s/it]\u001b[A\n",
      "53it [01:38,  2.66s/it]\u001b[A\n",
      "54it [01:40,  2.41s/it]\u001b[A\n",
      "55it [01:41,  2.24s/it]\u001b[A\n",
      "56it [01:43,  2.11s/it]\u001b[A\n",
      "57it [01:45,  1.92s/it]\u001b[A\n",
      "58it [01:47,  1.90s/it]\u001b[A\n",
      "59it [01:48,  1.83s/it]\u001b[A\n",
      "60it [01:49,  1.61s/it]\u001b[A\n",
      "61it [01:51,  1.67s/it]\u001b[A\n",
      "62it [01:53,  1.65s/it]\u001b[A\n",
      "63it [01:54,  1.63s/it]\u001b[A\n",
      "64it [01:56,  1.70s/it]\u001b[A\n",
      "65it [01:58,  1.71s/it]\u001b[A\n",
      "66it [01:59,  1.68s/it]\u001b[A\n",
      "67it [02:01,  1.77s/it]\u001b[A\n",
      "68it [02:03,  1.74s/it]\u001b[A\n",
      "69it [02:05,  1.69s/it]\u001b[A\n",
      "70it [02:06,  1.68s/it]\u001b[A\n",
      "71it [02:08,  1.63s/it]\u001b[A\n",
      "72it [02:10,  1.79s/it]\u001b[A\n",
      "73it [02:12,  1.86s/it]\u001b[A\n",
      "74it [02:14,  1.82s/it]\u001b[A\n",
      "75it [02:15,  1.65s/it]\u001b[A\n",
      "                       \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 276.5497, 'train_samples_per_second': 0.904, 'epoch': 1.0}    \n",
      "100%|█████████████████████████████████████████| 250/250 [04:36<00:00,  1.11s/it]\n",
      "Saving model checkpoint to output/hk01meta/4\n",
      "Configuration saved in output/hk01meta/4/config.json\n",
      "Model weights saved in output/hk01meta/4/pytorch_model.bin\n",
      "tokenizer config file saved in output/hk01meta/4/tokenizer_config.json\n",
      "Special tokens file saved in output/hk01meta/4/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                      =      1.0\n",
      "  init_mem_cpu_alloc_delta   =      0MB\n",
      "  init_mem_cpu_peaked_delta  =      0MB\n",
      "  init_mem_gpu_alloc_delta   =   1498MB\n",
      "  init_mem_gpu_peaked_delta  =      0MB\n",
      "  train_mem_cpu_alloc_delta  =      0MB\n",
      "  train_mem_cpu_peaked_delta =     37MB\n",
      "  train_mem_gpu_alloc_delta  =   3917MB\n",
      "  train_mem_gpu_peaked_delta =   8569MB\n",
      "  train_runtime              = 276.5497\n",
      "  train_samples              =     1000\n",
      "  train_samples_per_second   =    0.904\n",
      "The following columns in the test set  don't have a corresponding argument in `CPTForConditionalGeneration.forward` and have been ignored: token_type_ids.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 4\n",
      "100%|███████████████████████████████████████████| 25/25 [00:42<00:00,  1.70s/it]\n",
      "CPU times: user 4.52 s, sys: 1.46 s, total: 5.98 s\n",
      "Wall time: 6min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python run_gen_v2.py --model_path 'output/hk01meta/2' --dataset hk01meta --data_dir demo_data --epoch '1' --batch_size '4' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a077b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
