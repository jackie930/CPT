<!doctype html><html class=no-js lang=en-us prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta property="og:title" content="SpotBot Workshop"><meta property="og:type" content="website"><meta property="og:url" content><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name=generator content="Hugo 0.80.0"><meta name=description content="My AWS Workshop"><meta name=author content="Jane Architect"><link rel="shortcut icon" href=https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico type=image/ico><link rel=icon href=https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico type=image/ico><title>动手实验2: 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型 :: SpotBot Workshop</title><link href=./css/nucleus.css rel=stylesheet><link href=./css/fontawesome-all.min.css rel=stylesheet><link href=./css/hybrid.css rel=stylesheet><link href=./css/featherlight.min.css rel=stylesheet><link href=./css/perfect-scrollbar.min.css rel=stylesheet><link href=./css/auto-complete.css rel=stylesheet><link href=./css/atom-one-dark-reasonable.css rel=stylesheet><link href=./css/theme.css rel=stylesheet><link href=./css/hugo-theme.css rel=stylesheet><link href=./css/theme-aws.css rel=stylesheet><script src=./js/jquery-3.3.1.min.js></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}:not(pre)>code+span.copy-to-clipboard{display:none}</style></head><body data-url=./03bart.html><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><div><a href=./ title="Go home"><img style=vertical-align:middle src=./images/logo.png height=70px></a></div></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label><input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=./js/lunr.min.js></script><script type=text/javascript src=./js/auto-complete.js></script><script type=text/javascript>var baseurl="";</script><script type=text/javascript src=./js/search.js></script></div><div class=highlightable><ul class=topics><li data-nav-id=/01introduction.html title="what is text summary" class=dd-item><a href=./01introduction.html><b>1.</b> what is text summary
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/01introduction/100algorithm.html title="1.1 算法概述" class=dd-item><a href=./01introduction/100algorithm.html>1.1 算法概述
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/01introduction/200data.html title="1.2 数据集" class=dd-item><a href=./01introduction/200data.html>1.2 数据集
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/01introduction/300metrics.html title="1.3 评估指标" class=dd-item><a href=./01introduction/300metrics.html>1.3 评估指标
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/02textrank.html title="动手实验1: 基于Amazon SageMaker的TEXTRANK模型训练动手实验" class=dd-item><a href=./02textrank.html><b>2. </b>动手实验1: 基于Amazon SageMaker的TEXTRANK模型训练动手实验
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/02textrank/0201overview.html title=环境准备 class=dd-item><a href=./02textrank/0201overview.html><b>2.0</b>环境准备
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/02textrank/0210component.html title=texkrank模型实验 class=dd-item><a href=./02textrank/0210component.html><b>2.1</b> texkrank模型实验
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/03bart.html title="动手实验2: 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型" class="dd-item
parent
active"><a href=./03bart.html><b>3. </b>动手实验2: 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/03bart/0301train.html title=BART模型实验 class=dd-item><a href=./03bart/0301train.html><b>3.1</b> BART模型实验
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/04mt5.html title="动手实验3: 基于Amazon SageMaker的MT5中文摘要模型训练动手实验" class=dd-item><a href=./04mt5.html><b>4. </b>动手实验3: 基于Amazon SageMaker的MT5中文摘要模型训练动手实验
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/04mt5/0301train.html title=MT5模型实验 class=dd-item><a href=./04mt5/0301train.html><b>4.1</b>MT5模型实验
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/05cpt.html title="动手实验4: 基于Amazon SageMaker的CPT中文摘要模型训练动手实验型" class=dd-item><a href=./05cpt.html><b>5. </b>动手实验4: 基于Amazon SageMaker的CPT中文摘要模型训练动手实验型
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/05cpt/0501train.html title=CPT模型训练 class=dd-item><a href=./05cpt/0501train.html><b>5.1</b> CPT模型训练
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/05cpt/0502train.html title=CPT模型增强训练 class=dd-item><a href=./05cpt/0502train.html><b>5.2</b> CPT模型增强训练
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/05cpt/0503deploy.html title=CPT模型部署 class=dd-item><a href=./05cpt/0503deploy.html><b>5.3</b> CPT模型部署
<i class="fas fa-check read-icon"></i></a></li></ul></li></ul><section id=prefooter><hr><ul><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i>Clear History</a></li></ul></section><section id=footer><left><h5 class=copyright>&copy; 2019 Amazon Web Services, Inc. or its Affiliates. All rights reserved.<h5></left></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fa fa-bars"></i></a></span><span class=links><a href=./>AWS Datalab- 文档摘要</a> > 动手实验2: 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型</span></div></div></div><div id=chapter><div id=body-inner><h1>动手实验2: 基于Amazon SageMaker的Huggingface训练一个BART英文摘要模型</h1><h2 id=模型架构>模型架构</h2><p>BART架构由两个主要组件组成：编码器和解码器。</p><ul><li>使用BERT的编码器组件，该组件用于从两个方向对输入语句进行编码，以获得更多上下文信息。</li><li>BART使用了来自GPT的解码器组件，该解码器组件用于重构噪声输入。然而，单词只能在leftward上下文使用，所以它不能学习双向互动。</li></ul><p>除了解码器部分使用GeLU激活函数而非ReLU外，BART使用标准的序列到序列transformer架构，参数的初始化是从N(0.0, 0.02)开始的。BART有两种变体，基本模型在编码器和解码器中使用6层，而大型模型则每个使用12层。</p><p><img src=../pics/03bart/1.png alt></p><h2 id=模型介绍>模型介绍</h2><p>BART是一种采用序列到序列模型构建的降噪自编码器，适用于各种最终任务。它使用基于标准transformer的神经机器翻译架构。BART的预训练包括：</p><p>1）使用噪声函数破坏文本;
<img src=../pics/03bart/3.png alt></p><p>2）学习序列到序列模型以重建原始文本。</p><p>这些预训练步骤的主要优势在于：该模型可以灵活处理原始输入文本，并学会有效地重建文本。</p><p>当为文本生成进行微调（fine-tuned）时，BART提供了健壮的性能，并且在理解任务中也能很好地工作。</p><p>该模型的结果SOTA：
<img src=../pics/03bart/2.png alt></p><p>结果范例： BART在摘要任务上做得非常出色。以下示例摘要由BART生成。示例取自Wikinews文章。正如您所看到的，模型输出是流利且符合语法的英语。然而，模型输出也是高度抽象的，从输入中复制的短语很少
<img src=../pics/03bart/4.png alt></p><h2 id=reference>reference</h2><ul><li>paper： <a href=https://arxiv.org/pdf/1910.13461.pdf>https://arxiv.org/pdf/1910.13461.pdf</a></li><li>source code： <a href=https://github.com/pytorch/fairseq/tree/main/examples/bart>https://github.com/pytorch/fairseq/tree/main/examples/bart</a></li></ul><p>@article{lewis2019bart,
title = {BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension},
author = {Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and
Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov
and Luke Zettlemoyer },
journal={arXiv preprint arXiv:1910.13461},
year = {2019},
}</p><footer class=footline></footer></div></div></div><div id=navigation><a class="nav nav-prev" href=./02textrank/0210component.html title=texkrank模型实验><i class="fa fa-chevron-left"></i></a><a class="nav nav-next" href=./03bart/0301train.html title=BART模型实验 style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=./js/clipboard.min.js></script><script src=./js/perfect-scrollbar.min.js></script><script src=./js/perfect-scrollbar.jquery.min.js></script><script src=./js/jquery.sticky.js></script><script src=./js/featherlight.min.js></script><script src=./js/html5shiv-printshiv.min.js></script><script src=./js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script><script src=./js/modernizr.custom-3.6.0.js></script><script src=./js/learn.js></script><script src=./js/hugo-learn.js></script><link href=./mermaid/mermaid.css rel=stylesheet><script src=./mermaid/mermaid.js></script><script>mermaid.initialize({startOnLoad:true});</script></body></html>